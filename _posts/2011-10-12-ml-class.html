---
layout: post
title:  Personal notes for the stanford ml-class
tags: ai, stanford, lecture notes
---


<p>
I am following the Stanford <a href="http://www.ml-class.com">ml-class</a> online, and using this post for
keeping track of progress/notes/links etc. These notes are very rough,
mostly a personal log of progress&hellip;
</p>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">Lecture Notes </h2>
<div class="outline-text-2" id="text-1">


</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1">I. Introduction </h3>
<div class="outline-text-3" id="text-1-1">

<ul>
<li>Database Mining / Application that can't be programed by hand
        / Self-customizing programs (recommendations)
</li>
<li>ability to learn without being explicitly programmed 
</li>
<li>Supervised Learningq
<ul>
<li>Given some right answers
</li>
<li>Regression
</li>
<li>Classification - discrete value output/ feature (attibutes on
       which we are classifying)
</li>
</ul>

</li>
<li>Unsupervised Learning
<ul>
<li>No correct answers given
</li>
<li>Clustering
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2">Linear </h3>
<div class="outline-text-3" id="text-1-2">

<ul>
<li>Linear regression with one variable / univariate linear
     regression : hypothesis is a linear function!
</li>
<li>h(x) = &theta;<sub>0</sub> + &theta;<sub>1</sub>.x 
</li>
<li>Chose parameters so that h(x) satisfies the training set.
</li>
<li>Squared error cost fuction J(&theta;<sub>0</sub>, &theta;<sub>1</sub>)  = min &sum;[h(x) - y]<sup>2</sup>
</li>
</ul>

</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3">Gradient descent </h3>
<div class="outline-text-3" id="text-1-3">

<ul>
<li>alpha too small - gradient descent is slow
</li>
<li>alpha is too large - might miss minimum and never converge
</li>
<li>as we approach min - derivitative decreases so our step size is
     automatically decreasing
</li>
</ul>

</div>

</div>

<div id="outline-container-1-4" class="outline-3">
<h3 id="sec-1-4">Linear regression in one varioable </h3>
<div class="outline-text-3" id="text-1-4">

<ul>
<li>Apply gradient descent to minimize cost function
</li>
<li>Cons function is always a convex function, and therefore the
     function has only one global optimium (which is why gradient
     descent works!)
</li>
<li>Batch gradient descent ( uses all of the training data)
</li>
<li>Global equation method - numerical way to solve gradient descent
</li>
</ul>




</div>

</div>

<div id="outline-container-1-5" class="outline-3">
<h3 id="sec-1-5">Matrix Inverse / Transpose </h3>
<div class="outline-text-3" id="text-1-5">

<ul>
<li>If A has an inverse (only sq matrices) AA<sup>-1</sup> = A<sup>-1</sup>A = I
</li>
</ul>


</div>

</div>

<div id="outline-container-1-6" class="outline-3">
<h3 id="sec-1-6">Linear Regression with Multiple Variables </h3>
<div class="outline-text-3" id="text-1-6">

<ul>
<li>Mutliple features / multivariate linear regression
     n = number of features; x<sup>(i)</sup> = input features of i<sup>th</sup> training example.
</li>
<li>h (x) = &theta;<sub>0</sub> x<sub>0</sub> + &theta;<sub>1</sub> x<sub>1</sub> + &theta;<sub>2</sub> x<sub>2</sub> + &hellip; &theta;<sub>n</sub>
     x<sub>n</sub>
     define: x<sub>0</sub> = 1 = &theta;<sub>T</sub>x
</li>
<li>Cost function - J(&theta;<sub>0</sub>, &theta;<sub>1</sub>, &hellip; , &theta;<sub>n</sub>) = 1/2m
     &sum; (h<sub>&theta;</sub>(x<sup>(i)</sup>) - y<sup>(i)</sup>)<sup>2</sup>
</li>
<li>Gradient Descent : 
     &theta;<sub>j</sub> := &theta;<sub>j</sub> - &alpha; (partial derivate) J(&theta;)
</li>
<li>Feature Scaling
<ul>
<li>Make features on the same scale, causes faster convergence due
       to more circular contours. Get feature into -1 &lt;= x<sub>i</sub> &lt;= 1
</li>
<li>Mean normalization x<sub>i</sub> = x<sub>i</sub> - &mu;<sub>i</sub> 
</li>
</ul>

</li>
<li>How to choose &alpha;
<ul>
<li>Gradient desent not working - use smaller &alpha;.
</li>
<li>For sufficiently small &alpha; J(&theta;) would decrease on every iteration
</li>
<li>Too small &alpha; can be too slow
</li>
<li>Plot J(theat) v/s #iterations
</li>
</ul>

</li>
<li>Polynomial Regression : similar idea - except polynomial function
     for hypothesis - feature scaling can become a problem sometimes
</li>
<li>
<ul>
<li>Solve for &theta; analytically instead of
       iteratively (Gradient descent)
</li>
<li>Take derivatives and minimize
</li>
<li>min &theta; = (X<sup>T</sup>.X)<sup>-1</sup>X<sup>T</sup> y [x = feature matrix (m x n+1)]
</li>
<li>Feature scaling is irrelevant here
</li>
<li>Disadvantages
<ul>
<li>slow for large n (features) O(n<sup>3</sup>) for matrix computations
</li>
<li>n = 1000 (doable); n=10,000 kind of slow; n = 10<sup>6</sup> (no go)
</li>
<li>Doesnt work for all algorithms (we'll see later)
</li>
</ul>

</li>
</ul>

</li>
<li>Normal Equation and invertibility (advanced)
<ul>
<li>What if X<sup>TX</sup> is non-invertible? [Octave pinv vs inv]
</li>
<li>Some Causes
<ul>
<li>Redundant features i.e. linearly dependant features x<sub>1</sub> =
         &theta; x<sub>2</sub>
</li>
<li>Too many features (m &lt;= n) [Delete some features, use regularization]
</li>
</ul>

</li>
</ul>

</li>
<li>Octave notes
<ul>
<li>
</li>
</ul>

</li>
</ul>

</div>

</div>

<div id="outline-container-1-7" class="outline-3">
<h3 id="sec-1-7">Logistic Regression </h3>
<div class="outline-text-3" id="text-1-7">

<ul>
<li>Classification (Spam/Not Spam etc. )
</li>
<li>Binary classification
<ul>
<li>Run linear regression and use threshold as h<sub>&theta;</sub> (x) = 0.5?
</li>
<li>Linear regression doesn't work very well since the straigh line
       fit varies when applied to classification problem
</li>
<li>Logistic Regression 0 &lt;= h&theta; (x) &lt;= 1
</li>
<li>h&theta; (x) = g(&theta;<sup>T</sup> x) ; g(z) = 1 / 1 + e<sup>-z</sup>
</li>
<li>asymtotoes at zero/one (Sigmoid function/ logistic function)
</li>
<li>Decision Boundary
</li>
<li>How to chose &theta; - cost function
</li>
<li>cost function needs to be convex - linear regression cost
       square function won't work with the new h(&theta;)
</li>
<li>Cost(h<sub>&theta;</sub>(x), y) = -log(h<sub>&theta;</sub>(x)) if y = 1;
       -log(1-h<sub>&theta;</sub>(x)) y=0
</li>
<li>Cost = 0 if y = 1 h&theta;(x) ; cost = infinity P(y =
       1|x;&theta;) = 0, but y=1. We penalize wrong answers by large costs.
</li>
<li>Cost(h<sub>&theta;</sub>(x),y) = -y . log(h<sub>&theta;</sub>(x)) - (1-y)log(1-h<sub>&theta;</sub>(x))
</li>
<li>J(&theta;) = i/m ( &sum; cost())
</li>
<li>Advanced optimization
<ul>
<li>Conjugate gradient; BFGS; L-BFGS
</li>
<li>No need to pick &alpha;; converge faster; more complex
</li>
<li>fminunc(@costfunction, initialTheta, options) - ocatave
         function to minimize
</li>
</ul>

</li>
<li>Multiclass Classification
<ul>
<li>Email folder/tagging (y = small number of discrete values)
</li>
<li>Train a logistic regression classifer for each class i to
         predict the probability that y = i;
</li>
<li>Run all three classifies, and pick the max probability
</li>
</ul>

</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-8" class="outline-3">
<h3 id="sec-1-8">Overfitting </h3>
<div class="outline-text-3" id="text-1-8">


<ul>
<li>Higher order polynomial trying to fit training data
</li>
<li>Reduce number of features; model selecion algorithm
</li>
<li>Regularization
<ul>
<li>Keep all features; reduce mag/values of parameters &theta;<sub>j</sub>
</li>
<li>Lots of features contibuting to prediction y
</li>
<li>Penalize and make &theta;<sub>3</sub> and &theta;<sub>4</sub> small
</li>
<li>ex. cost + 1000&theta;<sub>3</sub><sup>2</sup> &hellip;
</li>
<li>essentially we end up with quadratic function
</li>
<li>If we have small values for parameters
<ul>
<li>'Simpler' hypothesis
</li>
<li>Less pront to overfitting
</li>
</ul>

</li>
<li>Modify cost function to + \lamda &sum;<sub>i=1</sub> &theta;<sub>j</sub> <sup>2</sup> (don't
       regularize &theta;<sub>0</sub> by convention)
</li>
<li>new gd = &theta;<sub>j</sub> (1- (&alpha; * \labda) / m) - &alpha; / m (cost
       function &hellip; ) ; reduces &theta;<sub>j</sub> since 1-.. &lt; 1
</li>
<li>For normal method using regularization gaurantees invertibility
       of the matrix! (pinv/ inv in octave&hellip;)
</li>
<li>Logistic regularization:
<ul>
<li>overfitting with lots of features etc.
</li>
<li>add the normal regularization penalizer
</li>
</ul>

</li>
</ul>

</li>
</ul>

</div>

</div>

<div id="outline-container-1-9" class="outline-3">
<h3 id="sec-1-9">Neural Networks Representation </h3>
<div class="outline-text-3" id="text-1-9">

<ul>
<li>Non - linear hypotheses
     Linear regression is problematic with lots of features for
     non-linear classifiers
</li>
<li>
</li>
</ul>


</div>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2">Homework Notes </h2>
<div class="outline-text-2" id="text-2">

</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3">Web Links </h2>
<div class="outline-text-2" id="text-3">

<ul>
<li><a href="http://www.reddit.com/r/mlclass">Reddit Discussions</a>
</li>
</ul>


</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4">Other </h2>
<div class="outline-text-2" id="text-4">


</div>
</div>
