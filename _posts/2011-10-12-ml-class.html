---
layout: post
title:  Personal notes for the stanford ml-class
tags: ai, stanford, lecture notes
---


<p>
I am following the Stanford <a href="http://www.ml-class.com">ml-class</a> online, and using this post for
keeping track of progress/notes/links etc. These notes are very rough,
mostly a personal log of progress&hellip;
</p>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1">Lecture Notes </h2>
<div class="outline-text-2" id="text-1">


</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1">I. Introduction </h3>
<div class="outline-text-3" id="text-1-1">

<ul>
<li>Database Mining / Application that can't be programed by hand
        / Self-customizing programs (recommendations)
</li>
<li>ability to learn without being explicitly programmed 
</li>
<li>Supervised Learningq
<ul>
<li>Given some right answers
</li>
<li>Regression
</li>
<li>Classification - discrete value output/ feature (attibutes on
       which we are classifying)
</li>
</ul>

</li>
<li>Unsupervised Learning
<ul>
<li>No correct answers given
</li>
<li>Clustering
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2">Linear </h3>
<div class="outline-text-3" id="text-1-2">

<ul>
<li>Linear regression with one variable / univariate linear
     regression : hypothesis is a linear function!
</li>
<li>h(x) = &theta;<sub>0</sub> + &theta;<sub>1</sub>.x 
</li>
<li>Chose parameters so that h(x) satisfies the training set.
</li>
<li>Squared error cost fuction J(&theta;<sub>0</sub>, &theta;<sub>1</sub>)  = min &sum;[h(x) - y]<sup>2</sup>
</li>
</ul>

</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3">Gradient descent </h3>
<div class="outline-text-3" id="text-1-3">

<ul>
<li>alpha too small - gradient descent is slow
</li>
<li>alpha is too large - might miss minimum and never converge
</li>
<li>as we approach min - derivitative decreases so our step size is
     automatically decreasing
</li>
</ul>

</div>

</div>

<div id="outline-container-1-4" class="outline-3">
<h3 id="sec-1-4">Linear regression in one varioable </h3>
<div class="outline-text-3" id="text-1-4">

<ul>
<li>Apply gradient descent to minimize cost function
</li>
<li>Cons function is always a convex function, and therefore the
     function has only one global optimium (which is why gradient
     descent works!)
</li>
<li>Batch gradient descent ( uses all of the training data)
</li>
<li>Global equation method - numerical way to solve gradient descent
</li>
</ul>




</div>

</div>

<div id="outline-container-1-5" class="outline-3">
<h3 id="sec-1-5">Matrix Inverse / Transpose </h3>
<div class="outline-text-3" id="text-1-5">

<ul>
<li>If A has an inverse (only sq matrices) AA<sup>-1</sup> = A<sup>-1</sup>A = I
</li>
</ul>

</div>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2">Homework Notes </h2>
<div class="outline-text-2" id="text-2">

</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3">Web Links </h2>
<div class="outline-text-2" id="text-3">

<ul>
<li><a href="http://www.reddit.com/r/mlclass">Reddit Discussions</a>
</li>
</ul>


</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4">Other </h2>
<div class="outline-text-2" id="text-4">


</div>
</div>
